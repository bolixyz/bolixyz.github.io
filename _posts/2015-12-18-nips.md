---
layout: post
title: NIPS Snippets
comments: true
---

NIPS is one of the premier ML conferences. As my job is more on application side of ML, I have never had the chance to join it in person. But every year, there are huge amount discussions during and after the conference on the Internet, which is also of great help. Following are some snippets I found interesting (thanks a lot for this [list](https://www.reddit.com/r/MachineLearning/comments/3x2ueg/nips_2015_overviews_collection/))

* Saddle Points vs Local Minima

In low-dimensional spaces local minima are the major impediment to optimizers reaching the global minimum. But in high-dimensional spaces, local minima are almost non-existent. Instead, there are saddle points: points which are a minimum in some directions but a maximum in others. Intuitively, in N dimensions, the odds of the curvatures all going the same way at a point is $$\frac{1}{2^N}$$. This gives an intuition for why gradient descent are effective at optimizing the thousands of weights in a neural net and don't get stuck in a local optimum. This also explains why momentum is helpful: it helps gradient descent escape from saddle points. ([Source](http://www.danvk.org/2015/12/12/nips-2015.html))

* Large vs Small nets

Due to the high cost of un-cached, off-chip memory reads, reducing the memory footprint of network models can be a huge performance win. It has been found by iteratively removing small weights from a model and retraining, they were able to remove +90% of the weights with zero loss of precision. This parallels an observation from transfer learning, that small networks are most effectively trained using the output of larger networks. ([Source](http://www.danvk.org/2015/12/12/nips-2015.html))

