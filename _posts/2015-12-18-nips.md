---
layout: post
title: NIPS Snippets
comments: true
---

NIPS is one of the premier ML conferences. As my job is more on application side of ML, I have never had the chance to join it in person. But every year, there are huge amount discussions during and after the conference on the Internet, which is also of great help. Following are some snippets I found interesting (thanks a lot for this [list](https://www.reddit.com/r/MachineLearning/comments/3x2ueg/nips_2015_overviews_collection/))

* Saddle Points vs Local Minima <br><br>
In low-dimensional spaces local minima are the major impediment to optimizers reaching the global minimum. But in high-dimensional spaces, local minima are almost non-existent. Instead, there are saddle points: points which are a minimum in some directions but a maximum in others. Intuitively, in N dimensions, the odds of the curvatures all going the same way at a point is $$\frac{1}{2^N}$$. This gives an intuition for why gradient descent are effective at optimizing the thousands of weights in a neural net and don't get stuck in a local optimum. This also explains why momentum is helpful: it helps gradient descent escape from saddle points. ([Source](http://www.danvk.org/2015/12/12/nips-2015.html))

* Large vs Small nets <br><br>
Due to the high cost of un-cached, off-chip memory reads, reducing the memory footprint of network models can be a huge performance win. It has been found by iteratively removing small weights from a model and retraining, they were able to remove +90% of the weights with zero loss of precision. This parallels an observation from transfer learning, that small networks are most effectively trained using the output of larger networks. ([Source](http://www.danvk.org/2015/12/12/nips-2015.html))

* Reinforcement Learning <br><br>
As pointed out in [this post](http://www.machinedlearnings.com/2015/12/nips-2015-review.html), RL is attracting more attentions. Personally, I haven't carefully follow this particular directon but the idea of rewards/feedbacks attracts me a lot. It's probably the time to catch it up. Following are some points, more exactly a reading list I found interesting from that post.<br>
  * Deepmind's work in [Arcade Learning Environment](http://www.arcadelearningenvironment.org/)
  * [Deep RL workshop](http://rll.berkeley.edu/deeprlworkshop/). The current big boost in RL performance could be mostly characterized as 1) decoding the screen better with convnets and 2) using multiple previous frames as input. However, a hard part of RL, namely, partial feedback over long action sequences, is not yet addressed. 

* Automated machine learning (AutoML) <br><br>
To eliminate the need for expertise in typical supervised learning setups. An interesting paper is [Efficient and Robust Automated Machine Learning](http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning) and the [AutoML challenge](http://codalab.org/AutoML) at the CIML workshop.

* Memory systems <br><br>
  * [End-to-End Memory Networks](http://papers.nips.cc/paper/5846-end-to-end-memory-networks)
  * the [RAM workshop](http://www.thespermwhale.com/jaseweston/ram/)
  * Attention as a mechanism for mitigating sample complexity: if you are not attending to somthing you are invariant to it, which greatly mitigates data requirements, assuming of course that you are ignoring irrelevant stuff. Is it somehow less expensive statistically to figure out what is important rather than how it is important, preseving precious data resources for the latter? 
  * [Learning Wake-Sleep Recurrent Attention Models](http://papers.nips.cc/paper/5861-learning-wake-sleep-recurrent-attention-models)

* Highway networks <br><br>
  * [Highway networks](http://people.idsia.ch/~rupesh/very_deep_learning/)
  * All existing deep networks can be considered highway networks with an uncountable number of identity transformation layers elided past a certain depth, i.e., incompletely optimized "infinitely deep" highway networks

* [Brain, Minds and Machines Symposium](http://www.mit.edu/~mnick/brains-minds-and-machines-2015/) <br><br>
  * Deep learning is very well-suited for pattern recognition and dealing with high-dimensional inputs.
  * But we need to combine this with other frameworks - for example probabilistic inference and simulation - to solve many problems that humans do with ease, such as Bayesian, probabilistic machine learning. 

* [Open AI](https://openai.com/blog/introducing-openai/)



